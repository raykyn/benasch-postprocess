#!/usr/bin/env python3

"""
This script transforms the annotations generated by flair into Standard-XML files.
Input is a list of Sentences/Docs, Annotations.
A list of Metadata Dicts can optionally be given.
"""

import json
from lxml import etree as et
import pprint as pp
import os


def transform_text(root, sentence):
    text_elem = et.SubElement(root, "Text")
    line_elem = et.SubElement(text_elem, "L", line_id="0")  # NOTE: Line splits get lost during tagging, but we could keep them by using [SEP]-markers which get removed for inference
    tokens = sentence.split()
    for t, token in enumerate(tokens):
        token_elem = et.SubElement(line_elem, "T", token_id=str(t))
        token_elem.text = token


def match_heads(annotations, conversion_file) -> list:
    """
    Sort all non-head tags by length.
    Each head is added to the shortest annotation it is contained within.
    Output warning if multiple heads are added to the same
    or no heads at all are added to a span that should have heads.
    or if a head cannot find a span where it is contained.
    Keyword head is hardcoded atm
    """
    non_heads_sorted = sorted([a for a in annotations if conversion_file["labels_full"][a["labels"][0]["value"]]["has_head"]], key=lambda x: x["token_end"] - x["token_start"])
    heads = [a for a in annotations if a["labels"][0]["value"] == "head"]
    heads_without_parent = []
    for head in heads:
        for non_head in non_heads_sorted:
            if head["token_start"] >= non_head["token_start"] and head["token_end"] <= non_head["token_end"]:
                if "head" in non_head:
                    non_head["head"].append(head)
                else:
                    non_head["head"] = [head]
                break
        else:
            heads_without_parent.append(head)

    spans_without_heads = []
    spans_with_multiple_heads = []
    for non_head in non_heads_sorted:
        if "head" not in non_head:
            spans_without_heads.append(non_head)
        elif len(non_head["head"]) > 1:
            spans_with_multiple_heads.append(non_head)
        else:
            pass
            # print(non_head["text"], ", ".join([h["text"] for h in non_head["head"]]))

    print("MULTIPLE HEADS:")
    pp.pprint(spans_with_multiple_heads)
    print("NO HEADS:")
    pp.pprint(spans_without_heads)
    print("HEADS WITHOUT (ALLOWED) PARENT SPANS:")
    pp.pprint(heads_without_parent)  # TODO: Add heads without parents as unk entity mentions


def transform_annotations(root, annotations, conv):
    # we need to match heads to other tags
    match_heads(annotations, conv)
    # sort annotations by their start index, end index
    annotations = sorted([a for a in annotations if a["labels"][0]["value"] != "head"], key=lambda x: (x["token_start"], -x["token_end"]))

    # create categories
    category_elems = {}
    for cat in conv["categories"]:
        category_elems[cat] = et.SubElement(root, cat)
    # append the annotations
    for anno in annotations:
        value = anno["labels"][0]["value"]
        category = conv["label_category"][value]  # category = Mentions zb
        defaults = conv["category_defaults"][category]
        anno_elem = et.SubElement(category_elems[category], defaults["tag"], defaults["attribs"])
        # add annotated info (labels full will need to be replaced by whatever algorithm is used to split labels)
        has_head = False
        for key, value in conv["labels_full"][value].items():
            if key == "has_head":
                has_head = value
            else:  # attrib overwrites
                anno_elem.set(key, value)
        # add start and end indices
        anno_elem.set("start", str(anno["token_start"]))
        anno_elem.set("end", str(anno["token_end"] + 1))
        
        # add head info and text
        if has_head:
            # now we need to resolve what happens if no head, multiple heads
            if "head" not in anno:
                # no head scenario
                anno_elem.set("head_start", "")
                anno_elem.set("head_end", "")
                anno_elem.set("head_text", "NO_HEAD_FOUND")
            elif len("head") == 1:
                # perfect scenario
                head = anno["head"]
                anno_elem.set("head_start", str(head["token_start"]))
                anno_elem.set("head_end", str(head["token_end"] + 1))
                anno_elem.set("head_text", head["text"])
            else:
                # multiple heads scenario
                # choose the one with the highest confidence
                head = max(anno["head"], key=lambda x: x["labels"][0]["confidence"])
                anno_elem.set("head_start", str(head["token_start"]))
                anno_elem.set("head_end", str(head["token_end"] + 1))
                anno_elem.set("head_text", head["text"])
        else:
            anno_elem.set("text", anno["text"])
        
    for i, elem in enumerate(category_elems["Mentions"]):
        elem.set("mention_id", str(i))
        

def filter_ignored_annotations(annotations, conv):
    for anno in annotations:
        if anno["labels"][0]["value"] in conv["ignore_labels"]:
            print("Ignoring annotation:", anno)
        else:
            yield anno


def transform(sentence, annotations, conversion_file, metadata=None):
    with open(conversion_file, mode="r", encoding="utf8") as inf:
        conv = json.load(inf)

    root = et.Element("XML")
    transform_text(root, sentence)
    annotations = list(filter_ignored_annotations(annotations, conv))
    convert_annotation_to_token_idx(root, annotations)
    transform_annotations(root, annotations, conv)
    # print(et.tostring(root, pretty_print=True))

    return root


def convert_annotation_to_token_idx(root, annotation):
    tokens = root.findall(".//T")
    start_to_token_idx = {}
    end_to_token_idx = {}
    current_idx = 0
    for token in tokens:
        start_to_token_idx[current_idx] = token.get("token_id")
        current_idx += len(token.text)
        end_to_token_idx[current_idx] = token.get("token_id")
        current_idx += 1
    for anno in annotation:
        anno["token_start"] = int(start_to_token_idx[anno.get("fixed_start_pos")])
        # Inception performs an implicit tokenization, which allows annotations
        # to be set outside our own preprocessing. This can lead to annotations
        # ending inside tokens as defined by our preprocessing/system
        # to circumvent this problem, we simply stretch the tag to the end of the token
        end = anno.get("fixed_end_pos")
        while end not in end_to_token_idx:
            end += 1
        anno["token_end"] = int(end_to_token_idx[end])
        

def read_sentence_file(sentence_file):
    sentences = []
    with open(sentence_file, mode="r", encoding="utf8") as inf:
        for line in inf:
            sentences.append(line)
    return sentences


def read_annotation_file(annotation_file):
    with open(annotation_file, mode="r", encoding="utf8") as inf:
        return json.load(inf)


if __name__ == "__main__":
    outfolder = "../auto_tagged/"

    sentence_file = "../data/from_rec_flair/test_data_plain.txt"
    anntation_file = "../data/from_rec_flair/results.json"
    conversion_file = "./conversion_files/from_rec_flair/only_entity_types.json"

    sents = read_sentence_file(sentence_file)
    annos = read_annotation_file(anntation_file)

    for i, (sent, anno) in enumerate(zip(sents, annos)):
        outpath = os.path.join(outfolder, f"{i}.xml")
        std_xml = et.ElementTree(transform(sent, anno, conversion_file))
        std_xml.write(outpath, pretty_print=True, xml_declaration=True, encoding='UTF-8')
        