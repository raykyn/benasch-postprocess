#!/usr/bin/env python3

"""
This script transforms the annotations generated by flair into Standard-XML files.
Input is a list of Sentences/Docs, Annotations and a json with rules for the tag resolution in regex-expression form.
A list of Metadata Dicts can optionally be given.

This should replace 'from_rec_flair' when it's finished.

the conversion file works like this:
on every level, the file may either specify a regex expression
under the key "regex" or it may present multiple tags as keys.
If a "regex" key is present all remaining tag will be read 
with that expression and values will be assigned accordingly.
If other keys are presented instead, the tag is split at the first
period and evaluated against the keys (according to regex syntax => Nope, pretty complicated).
See the included conversion file as an example.
A certain amount of the conversation process is hardcoded,
such as the fact that Mentions and Values exist etc.
"""


from datetime import date
import json
from lxml import etree as et
import pprint as pp
import os
import re


# hardcoded, Benasch-related
CATEGORIES = {
    "Reference": "Mentions",
    "Attribute": "Mentions",
    "Value": "Values",
    "List": "Mentions"
}

HAS_HEADS = ["Reference", "Attribute"]

DEFAULTS = {
        "List": {
            "attribs": {
                "subtype": "unk",
                "entity_types": "unk"
            }
        },
        "Reference": {
            "attribs": {
                "mention_id": "0",
                "mention_type": "unk",
                "mention_subtype": "unk",
                "entity_type": "unk",
                "numerus": "sgl",
                "specificity": "spc"
            }
        },
        "Attribute": {
            "attribs": {
                "mention_id": "0",
                "mention_type": "unk",
                "mention_subtype": "unk",
                "entity_type": "unk",
                "numerus": "sgl",
                "specificity": "spc"
            }
        },
        "Descriptor": {
            "attribs": {
                "desc_type": "unk"
            }
        },
        "Value": {
            "attribs": {
                "value_id": "0",
                "value_type": "unk"
            }
        },
        "Relation": {
            "attribs": {
                "rel_type": "unk",
                "tense": "pres"
            }
        }
    }

OTHER_FIELDS = {
        "numerus": ["sgl", "grp", "neg"],
        "specificity": ["spc", "uspc"],
        "tense": ["pres", "past", "fut"],
        "mention_type": ["pro"]
    }


def solve_other(tag):
    for t in tag.split("."):
        if not t:
            continue
        for k, v in OTHER_FIELDS.items():
            if t in v:
                yield k, t


def transform_text(root, sentence):
    text_elem = root.find("Text")
    lines = sentence.split(" [sep] ")
    for i, line in enumerate(lines):
        line_elem = et.SubElement(text_elem, "L", line_id=str(i))  # NOTE: Line splits get lost during tagging, but we could keep them by using [SEP]-markers which get removed for inference
        tokens = line.split()
        for t, token in enumerate(tokens):
            token_elem = et.SubElement(line_elem, "T", token_id=str(t))
            token_elem.text = token


def convert_annotation_to_token_idx(root, annotation):
    tokens = root.findall(".//T")
    start_to_token_idx = {}
    end_to_token_idx = {}
    current_idx = 0
    for token in tokens:
        start_to_token_idx[current_idx] = token.get("token_id")
        current_idx += len(token.text)
        end_to_token_idx[current_idx] = token.get("token_id")
        current_idx += 1
    for anno in annotation:
        anno["token_start"] = int(start_to_token_idx[anno.get("fixed_start_pos")])
        anno["token_end"] = int(end_to_token_idx[anno.get("fixed_end_pos")])


def match_heads(annotations, conversion_file) -> list:
    """
    Sort all non-head tags by length.
    Each head is added to the shortest annotation it is contained within.
    Output warning if multiple heads are added to the same
    or no heads at all are added to a span that should have heads.
    or if a head cannot find a span where it is contained.
    Keyword head is hardcoded atm
    """
    non_heads_sorted = sorted([a for a in annotations if conversion_file[a["labels"][0]["value"].split(".")[0]]["type"] in HAS_HEADS], key=lambda x: x["token_end"] - x["token_start"])
    heads = [a for a in annotations if a["labels"][0]["value"] == "head"]
    heads_without_parent = []
    for head in heads:
        for non_head in non_heads_sorted:
            if head["token_start"] >= non_head["token_start"] and head["token_end"] <= non_head["token_end"]:
                if "head" in non_head:
                    non_head["head"].append(head)
                else:
                    non_head["head"] = [head]
                break
        else:
            heads_without_parent.append(head)

    spans_without_heads = []
    spans_with_multiple_heads = []
    for non_head in non_heads_sorted:
        if "head" not in non_head:
            spans_without_heads.append(non_head)
        elif len(non_head["head"]) > 1:
            spans_with_multiple_heads.append(non_head)
        else:
            pass
            # print(non_head["text"], ", ".join([h["text"] for h in non_head["head"]]))

    print("MULTIPLE HEADS:")
    pp.pprint(spans_with_multiple_heads)
    print("NO HEADS:")
    pp.pprint(spans_without_heads)
    print("HEADS WITHOUT (ALLOWED) PARENT SPANS:")
    pp.pprint(heads_without_parent)  # TODO: Add heads without parents as unk entity mentions


def transform_annotations(root, annotations, conv):
    # we need to match heads to other tags
    match_heads(annotations, conv)
    # sort annotations by their start index, end index
    annotations = sorted([a for a in annotations if a["labels"][0]["value"] != "head"], key=lambda x: (x["token_start"], -x["token_end"]))

    # append the annotations
    for anno in annotations:
        value = anno["labels"][0]["value"]

        conv_cat = conv[value.split(".")[0]]
        category = CATEGORIES[conv_cat["type"]]

        anno_elem = et.SubElement(root.find(category), conv_cat["type"], DEFAULTS[conv_cat["type"]]["attribs"])
        
        # add annotated info by using the regex
        regex = conv_cat["regex"]  # at the moment all tags use regex
        match = re.match(regex, value, flags=re.I)
        for k, v in match.groupdict().items():
            if not v:
                continue
            if k != "other":
                anno_elem.set(k, v)
            else:
                for ko, vo in solve_other(v):
                    anno_elem.set(ko, vo)

        # add start and end indices
        anno_elem.set("start", str(anno["token_start"]))
        anno_elem.set("end", str(anno["token_end"] + 1))
        
        # add head info and text
        if conv_cat["type"] in HAS_HEADS:
            # now we need to resolve what happens if no head, multiple heads
            if "head" not in anno:
                # no head scenario
                anno_elem.set("head_start", "")
                anno_elem.set("head_end", "")
                anno_elem.set("head_text", "NO_HEAD_FOUND")
            elif len("head") == 1:
                # perfect scenario
                head = anno["head"]
                anno_elem.set("head_start", str(head["token_start"]))
                anno_elem.set("head_end", str(head["token_end"] + 1))
                anno_elem.set("head_text", head["text"])
            else:
                # multiple heads scenario
                # choose the one with the highest confidence
                head = max(anno["head"], key=lambda x: x["labels"][0]["confidence"])
                anno_elem.set("head_start", str(head["token_start"]))
                anno_elem.set("head_end", str(head["token_end"] + 1))
                anno_elem.set("head_text", head["text"])
        else:
            anno_elem.set("text", anno["text"])
        
    for i, elem in enumerate(root.find("Mentions")):
        elem.set("mention_id", str(i))

    for i, elem in enumerate(root.find("Values")):
        elem.set("value_id", str(i))


def transform(sentence, annotations, conversion_file, metadata=None):
    with open(conversion_file, mode="r", encoding="utf8") as inf:
        conv = json.load(inf)

    root = et.parse("std_template.xml").getroot()
    transform_text(root, sentence)
    # TODO: ignore certain tags as part of the tag resolution json
    # annotations = list(filter_ignored_annotations(annotations, conv))
    convert_annotation_to_token_idx(root, annotations)
    transform_annotations(root, annotations, conv)
    # print(et.tostring(root, pretty_print=True))

    return et.ElementTree(root)


def read_sentence_file(sentence_file):
    sentences = []
    with open(sentence_file, mode="r", encoding="utf8") as inf:
        for line in inf:
            sentences.append(line)
    return sentences


def read_sentences_from_pd_json(infile):
    with open(infile, mode="r", encoding="utf8") as inf:
        for entry in json.load(inf)["data"]:
            # access tokenized text
            yield entry, entry["tokenized_text"]
    

def read_annotation_file(annotation_file):
    """
    Comes in the form of a jsonl file, one line per sentence.
    """
    with open(annotation_file, mode="r", encoding="utf8") as inf:
        for line in inf:
            yield json.loads(line)


def add_metadata(xmltree, metadata):
    print(metadata)
    root = xmltree.getroot()
    metadata_node = et.Element("Metadata")
    root.insert(0, metadata_node)

    fileDesc = et.SubElement(metadata_node, "FileDesc")
    title = et.SubElement(fileDesc, "Title")
    if len(metadata["seiten"].split(", ")) > 1:
        pages = metadata["seiten"].split(", ")[0] + "-" + metadata["seiten"].split(", ")[-1]
    else:
        pages = metadata["seiten"]
    title.text = os.path.basename(metadata["dossier"] + "_" + pages)

    settingDesc = et.SubElement(metadata_node, "SettingDesc")
    time = et.SubElement(settingDesc, "Time")
    time.text = str(int(metadata["jahr"]))

    changelog = et.SubElement(metadata_node, "ChangeLog")
    change = et.SubElement(changelog, "Change", when=str(date.today()), who="IP")
    change.text = "Postprocessing of automatically annotated document. Annotation was performed by using Flair Recursive Algorithm developed by IP, using model 'specific_full'."


if __name__ == "__main__":
    outfolder = "./out/hgb_specific_full/"

    data_folder = "./data/hgb_corpus/"
    #sentence_file = "../data/from_rec_flair/test_data_plain.txt"
    corpus_file = data_folder + "hgb_corpus.json"
    anntation_file = data_folder + "annotated.jsonl"
    conversion_file = data_folder + "conv_specific_full.json"

    if corpus_file.endswith(".json"):
        meta_sents = read_sentences_from_pd_json(corpus_file)  # maybe also immediately read metadata from here
    else:
        raise NotImplementedError
    #sents = read_sentence_file(sentence_file)
    annos = read_annotation_file(anntation_file)

    for i, ((m, sent), anno) in enumerate(zip(meta_sents, annos)):
        #pp.pprint(sent)
        #pp.pprint(anno)
        print(m["entryid"])

        outpath = os.path.join(outfolder, f"{i}.xml")
        std_xml = transform(sent, anno, conversion_file)

        # insert metadata

        add_metadata(std_xml, m)

        std_xml.write(outpath, pretty_print=True, xml_declaration=True, encoding='UTF-8')